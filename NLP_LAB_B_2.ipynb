{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbaMd51To/Tcq7VJP6Imzw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2403a52270-commits/nlp/blob/main/NLP_LAB_B_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zNAhOhqkVPER"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------Tokenizing------------------"
      ],
      "metadata": {
        "id": "WNhCFZ-98RlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health_care = \"\"\"This activity focuses on applying preprocessing techniques to sensitive medical or healthcare-related text using Natural Language Processing tools. A simplified medical text corpus, such as PubMed abstracts or curated health articles, is first loaded into a Google Colab environment running Python 3.x with the required libraries NLTK and spaCy installed. The text is then preprocessed by performing sentence and word tokenization using both NLTK and spaCy to break the medical content into structured linguistic units. Further preprocessing includes applying stemming to reduce words to their root forms and lemmatization to convert medical terms into their meaningful dictionary base forms. The outputs of stemming and lemmatization are compared to highlight their differences, showing that stemming may distort medical terms while lemmatization preserves semantic and clinical meaning. This comparison leads to a discussion emphasizing why lemmatization is critical in healthcare NLP, as accurate terminology is essential for tasks such as clinical text analysis, information retrieval, medical coding, and decision support systems. The complete workflow is documented in a Google Colab notebook with proper headings and a discussion section at the end, and the final notebook is submitted as a PDF.\"\"\""
      ],
      "metadata": {
        "id": "Szp8fegeXNAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(health_care)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWOvRNsYbHD2",
        "outputId": "c6f75658-9693-4cc6-da68-74d370f8d978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'activity',\n",
              " 'focuses',\n",
              " 'on',\n",
              " 'applying',\n",
              " 'preprocessing',\n",
              " 'techniques',\n",
              " 'to',\n",
              " 'sensitive',\n",
              " 'medical',\n",
              " 'or',\n",
              " 'healthcare-related',\n",
              " 'text',\n",
              " 'using',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " 'tools',\n",
              " '.',\n",
              " 'A',\n",
              " 'simplified',\n",
              " 'medical',\n",
              " 'text',\n",
              " 'corpus',\n",
              " ',',\n",
              " 'such',\n",
              " 'as',\n",
              " 'PubMed',\n",
              " 'abstracts',\n",
              " 'or',\n",
              " 'curated',\n",
              " 'health',\n",
              " 'articles',\n",
              " ',',\n",
              " 'is',\n",
              " 'first',\n",
              " 'loaded',\n",
              " 'into',\n",
              " 'a',\n",
              " 'Google',\n",
              " 'Colab',\n",
              " 'environment',\n",
              " 'running',\n",
              " 'Python',\n",
              " '3.x',\n",
              " 'with',\n",
              " 'the',\n",
              " 'required',\n",
              " 'libraries',\n",
              " 'NLTK',\n",
              " 'and',\n",
              " 'spaCy',\n",
              " 'installed',\n",
              " '.',\n",
              " 'The',\n",
              " 'text',\n",
              " 'is',\n",
              " 'then',\n",
              " 'preprocessed',\n",
              " 'by',\n",
              " 'performing',\n",
              " 'sentence',\n",
              " 'and',\n",
              " 'word',\n",
              " 'tokenization',\n",
              " 'using',\n",
              " 'both',\n",
              " 'NLTK',\n",
              " 'and',\n",
              " 'spaCy',\n",
              " 'to',\n",
              " 'break',\n",
              " 'the',\n",
              " 'medical',\n",
              " 'content',\n",
              " 'into',\n",
              " 'structured',\n",
              " 'linguistic',\n",
              " 'units',\n",
              " '.',\n",
              " 'Further',\n",
              " 'preprocessing',\n",
              " 'includes',\n",
              " 'applying',\n",
              " 'stemming',\n",
              " 'to',\n",
              " 'reduce',\n",
              " 'words',\n",
              " 'to',\n",
              " 'their',\n",
              " 'root',\n",
              " 'forms',\n",
              " 'and',\n",
              " 'lemmatization',\n",
              " 'to',\n",
              " 'convert',\n",
              " 'medical',\n",
              " 'terms',\n",
              " 'into',\n",
              " 'their',\n",
              " 'meaningful',\n",
              " 'dictionary',\n",
              " 'base',\n",
              " 'forms',\n",
              " '.',\n",
              " 'The',\n",
              " 'outputs',\n",
              " 'of',\n",
              " 'stemming',\n",
              " 'and',\n",
              " 'lemmatization',\n",
              " 'are',\n",
              " 'compared',\n",
              " 'to',\n",
              " 'highlight',\n",
              " 'their',\n",
              " 'differences',\n",
              " ',',\n",
              " 'showing',\n",
              " 'that',\n",
              " 'stemming',\n",
              " 'may',\n",
              " 'distort',\n",
              " 'medical',\n",
              " 'terms',\n",
              " 'while',\n",
              " 'lemmatization',\n",
              " 'preserves',\n",
              " 'semantic',\n",
              " 'and',\n",
              " 'clinical',\n",
              " 'meaning',\n",
              " '.',\n",
              " 'This',\n",
              " 'comparison',\n",
              " 'leads',\n",
              " 'to',\n",
              " 'a',\n",
              " 'discussion',\n",
              " 'emphasizing',\n",
              " 'why',\n",
              " 'lemmatization',\n",
              " 'is',\n",
              " 'critical',\n",
              " 'in',\n",
              " 'healthcare',\n",
              " 'NLP',\n",
              " ',',\n",
              " 'as',\n",
              " 'accurate',\n",
              " 'terminology',\n",
              " 'is',\n",
              " 'essential',\n",
              " 'for',\n",
              " 'tasks',\n",
              " 'such',\n",
              " 'as',\n",
              " 'clinical',\n",
              " 'text',\n",
              " 'analysis',\n",
              " ',',\n",
              " 'information',\n",
              " 'retrieval',\n",
              " ',',\n",
              " 'medical',\n",
              " 'coding',\n",
              " ',',\n",
              " 'and',\n",
              " 'decision',\n",
              " 'support',\n",
              " 'systems',\n",
              " '.',\n",
              " 'The',\n",
              " 'complete',\n",
              " 'workflow',\n",
              " 'is',\n",
              " 'documented',\n",
              " 'in',\n",
              " 'a',\n",
              " 'Google',\n",
              " 'Colab',\n",
              " 'notebook',\n",
              " 'with',\n",
              " 'proper',\n",
              " 'headings',\n",
              " 'and',\n",
              " 'a',\n",
              " 'discussion',\n",
              " 'section',\n",
              " 'at',\n",
              " 'the',\n",
              " 'end',\n",
              " ',',\n",
              " 'and',\n",
              " 'the',\n",
              " 'final',\n",
              " 'notebook',\n",
              " 'is',\n",
              " 'submitted',\n",
              " 'as',\n",
              " 'a',\n",
              " 'PDF',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(health_care)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-nfW3V1c2ZR",
        "outputId": "0ecf1468-59bc-46d2-bee8-8820ab1d0dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This activity focuses on applying preprocessing techniques to sensitive medical or healthcare-related text using Natural Language Processing tools.',\n",
              " 'A simplified medical text corpus, such as PubMed abstracts or curated health articles, is first loaded into a Google Colab environment running Python 3.x with the required libraries NLTK and spaCy installed.',\n",
              " 'The text is then preprocessed by performing sentence and word tokenization using both NLTK and spaCy to break the medical content into structured linguistic units.',\n",
              " 'Further preprocessing includes applying stemming to reduce words to their root forms and lemmatization to convert medical terms into their meaningful dictionary base forms.',\n",
              " 'The outputs of stemming and lemmatization are compared to highlight their differences, showing that stemming may distort medical terms while lemmatization preserves semantic and clinical meaning.',\n",
              " 'This comparison leads to a discussion emphasizing why lemmatization is critical in healthcare NLP, as accurate terminology is essential for tasks such as clinical text analysis, information retrieval, medical coding, and decision support systems.',\n",
              " 'The complete workflow is documented in a Google Colab notebook with proper headings and a discussion section at the end, and the final notebook is submitted as a PDF.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------Filterinhg Stop Words------------------"
      ],
      "metadata": {
        "id": "MfxcH-FL9CeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca2tC9hwddrW",
        "outputId": "1e6114bf-ea87-45bb-826a-c16c046f4318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_quote = word_tokenize (health_care)\n",
        "words_in_quote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEfIp5Nwd1rY",
        "outputId": "47df63b9-b78b-4e92-b1cc-5b2e4ab3d1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'activity',\n",
              " 'focuses',\n",
              " 'on',\n",
              " 'applying',\n",
              " 'preprocessing',\n",
              " 'techniques',\n",
              " 'to',\n",
              " 'sensitive',\n",
              " 'medical',\n",
              " 'or',\n",
              " 'healthcare-related',\n",
              " 'text',\n",
              " 'using',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " 'tools',\n",
              " '.',\n",
              " 'A',\n",
              " 'simplified',\n",
              " 'medical',\n",
              " 'text',\n",
              " 'corpus',\n",
              " ',',\n",
              " 'such',\n",
              " 'as',\n",
              " 'PubMed',\n",
              " 'abstracts',\n",
              " 'or',\n",
              " 'curated',\n",
              " 'health',\n",
              " 'articles',\n",
              " ',',\n",
              " 'is',\n",
              " 'first',\n",
              " 'loaded',\n",
              " 'into',\n",
              " 'a',\n",
              " 'Google',\n",
              " 'Colab',\n",
              " 'environment',\n",
              " 'running',\n",
              " 'Python',\n",
              " '3.x',\n",
              " 'with',\n",
              " 'the',\n",
              " 'required',\n",
              " 'libraries',\n",
              " 'NLTK',\n",
              " 'and',\n",
              " 'spaCy',\n",
              " 'installed',\n",
              " '.',\n",
              " 'The',\n",
              " 'text',\n",
              " 'is',\n",
              " 'then',\n",
              " 'preprocessed',\n",
              " 'by',\n",
              " 'performing',\n",
              " 'sentence',\n",
              " 'and',\n",
              " 'word',\n",
              " 'tokenization',\n",
              " 'using',\n",
              " 'both',\n",
              " 'NLTK',\n",
              " 'and',\n",
              " 'spaCy',\n",
              " 'to',\n",
              " 'break',\n",
              " 'the',\n",
              " 'medical',\n",
              " 'content',\n",
              " 'into',\n",
              " 'structured',\n",
              " 'linguistic',\n",
              " 'units',\n",
              " '.',\n",
              " 'Further',\n",
              " 'preprocessing',\n",
              " 'includes',\n",
              " 'applying',\n",
              " 'stemming',\n",
              " 'to',\n",
              " 'reduce',\n",
              " 'words',\n",
              " 'to',\n",
              " 'their',\n",
              " 'root',\n",
              " 'forms',\n",
              " 'and',\n",
              " 'lemmatization',\n",
              " 'to',\n",
              " 'convert',\n",
              " 'medical',\n",
              " 'terms',\n",
              " 'into',\n",
              " 'their',\n",
              " 'meaningful',\n",
              " 'dictionary',\n",
              " 'base',\n",
              " 'forms',\n",
              " '.',\n",
              " 'The',\n",
              " 'outputs',\n",
              " 'of',\n",
              " 'stemming',\n",
              " 'and',\n",
              " 'lemmatization',\n",
              " 'are',\n",
              " 'compared',\n",
              " 'to',\n",
              " 'highlight',\n",
              " 'their',\n",
              " 'differences',\n",
              " ',',\n",
              " 'showing',\n",
              " 'that',\n",
              " 'stemming',\n",
              " 'may',\n",
              " 'distort',\n",
              " 'medical',\n",
              " 'terms',\n",
              " 'while',\n",
              " 'lemmatization',\n",
              " 'preserves',\n",
              " 'semantic',\n",
              " 'and',\n",
              " 'clinical',\n",
              " 'meaning',\n",
              " '.',\n",
              " 'This',\n",
              " 'comparison',\n",
              " 'leads',\n",
              " 'to',\n",
              " 'a',\n",
              " 'discussion',\n",
              " 'emphasizing',\n",
              " 'why',\n",
              " 'lemmatization',\n",
              " 'is',\n",
              " 'critical',\n",
              " 'in',\n",
              " 'healthcare',\n",
              " 'NLP',\n",
              " ',',\n",
              " 'as',\n",
              " 'accurate',\n",
              " 'terminology',\n",
              " 'is',\n",
              " 'essential',\n",
              " 'for',\n",
              " 'tasks',\n",
              " 'such',\n",
              " 'as',\n",
              " 'clinical',\n",
              " 'text',\n",
              " 'analysis',\n",
              " ',',\n",
              " 'information',\n",
              " 'retrieval',\n",
              " ',',\n",
              " 'medical',\n",
              " 'coding',\n",
              " ',',\n",
              " 'and',\n",
              " 'decision',\n",
              " 'support',\n",
              " 'systems',\n",
              " '.',\n",
              " 'The',\n",
              " 'complete',\n",
              " 'workflow',\n",
              " 'is',\n",
              " 'documented',\n",
              " 'in',\n",
              " 'a',\n",
              " 'Google',\n",
              " 'Colab',\n",
              " 'notebook',\n",
              " 'with',\n",
              " 'proper',\n",
              " 'headings',\n",
              " 'and',\n",
              " 'a',\n",
              " 'discussion',\n",
              " 'section',\n",
              " 'at',\n",
              " 'the',\n",
              " 'end',\n",
              " ',',\n",
              " 'and',\n",
              " 'the',\n",
              " 'final',\n",
              " 'notebook',\n",
              " 'is',\n",
              " 'submitted',\n",
              " 'as',\n",
              " 'a',\n",
              " 'PDF',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "health_care = \"\"\"This activity focuses on applying preprocessing techniques to sensitive medical or healthcare-related text using Natural Language Processing tools. A simplified medical text corpus, such as PubMed abstracts or curated health articles, is first loaded into a Google Colab environment running Python 3.x with the required libraries NLTK and spaCy installed. The text is then preprocessed by performing sentence and word tokenization using both NLTK and spaCy to break the medical content into structured linguistic units. Further preprocessing includes applying stemming to reduce words to their root forms and lemmatization to convert medical terms into their meaningful dictionary base forms. The outputs of stemming and lemmatization are compared to highlight their differences, showing that stemming may distort medical terms while lemmatization preserves semantic and clinical meaning. This comparison leads to a discussion emphasizing why lemmatization is critical in healthcare NLP, as accurate terminology is essential for tasks such as clinical text analysis, information retrieval, medical coding, and decision support systems. The complete workflow is documented in a Google Colab notebook with proper headings and a discussion section at the end, and the final notebook is submitted as a PDF.\"\"\"\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "words_in_quote = word_tokenize(health_care)\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_list = []\n",
        "for word in words_in_quote:\n",
        "    if word.casefold() not in stop_words:\n",
        "        filtered_list.append(word)\n",
        "filtered_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y5i8JvhfYkM",
        "outputId": "0141a0da-2e92-4baf-a870-8bb360528b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['activity',\n",
              " 'focuses',\n",
              " 'applying',\n",
              " 'preprocessing',\n",
              " 'techniques',\n",
              " 'sensitive',\n",
              " 'medical',\n",
              " 'healthcare-related',\n",
              " 'text',\n",
              " 'using',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " 'tools',\n",
              " '.',\n",
              " 'simplified',\n",
              " 'medical',\n",
              " 'text',\n",
              " 'corpus',\n",
              " ',',\n",
              " 'PubMed',\n",
              " 'abstracts',\n",
              " 'curated',\n",
              " 'health',\n",
              " 'articles',\n",
              " ',',\n",
              " 'first',\n",
              " 'loaded',\n",
              " 'Google',\n",
              " 'Colab',\n",
              " 'environment',\n",
              " 'running',\n",
              " 'Python',\n",
              " '3.x',\n",
              " 'required',\n",
              " 'libraries',\n",
              " 'NLTK',\n",
              " 'spaCy',\n",
              " 'installed',\n",
              " '.',\n",
              " 'text',\n",
              " 'preprocessed',\n",
              " 'performing',\n",
              " 'sentence',\n",
              " 'word',\n",
              " 'tokenization',\n",
              " 'using',\n",
              " 'NLTK',\n",
              " 'spaCy',\n",
              " 'break',\n",
              " 'medical',\n",
              " 'content',\n",
              " 'structured',\n",
              " 'linguistic',\n",
              " 'units',\n",
              " '.',\n",
              " 'preprocessing',\n",
              " 'includes',\n",
              " 'applying',\n",
              " 'stemming',\n",
              " 'reduce',\n",
              " 'words',\n",
              " 'root',\n",
              " 'forms',\n",
              " 'lemmatization',\n",
              " 'convert',\n",
              " 'medical',\n",
              " 'terms',\n",
              " 'meaningful',\n",
              " 'dictionary',\n",
              " 'base',\n",
              " 'forms',\n",
              " '.',\n",
              " 'outputs',\n",
              " 'stemming',\n",
              " 'lemmatization',\n",
              " 'compared',\n",
              " 'highlight',\n",
              " 'differences',\n",
              " ',',\n",
              " 'showing',\n",
              " 'stemming',\n",
              " 'may',\n",
              " 'distort',\n",
              " 'medical',\n",
              " 'terms',\n",
              " 'lemmatization',\n",
              " 'preserves',\n",
              " 'semantic',\n",
              " 'clinical',\n",
              " 'meaning',\n",
              " '.',\n",
              " 'comparison',\n",
              " 'leads',\n",
              " 'discussion',\n",
              " 'emphasizing',\n",
              " 'lemmatization',\n",
              " 'critical',\n",
              " 'healthcare',\n",
              " 'NLP',\n",
              " ',',\n",
              " 'accurate',\n",
              " 'terminology',\n",
              " 'essential',\n",
              " 'tasks',\n",
              " 'clinical',\n",
              " 'text',\n",
              " 'analysis',\n",
              " ',',\n",
              " 'information',\n",
              " 'retrieval',\n",
              " ',',\n",
              " 'medical',\n",
              " 'coding',\n",
              " ',',\n",
              " 'decision',\n",
              " 'support',\n",
              " 'systems',\n",
              " '.',\n",
              " 'complete',\n",
              " 'workflow',\n",
              " 'documented',\n",
              " 'Google',\n",
              " 'Colab',\n",
              " 'notebook',\n",
              " 'proper',\n",
              " 'headings',\n",
              " 'discussion',\n",
              " 'section',\n",
              " 'end',\n",
              " ',',\n",
              " 'final',\n",
              " 'notebook',\n",
              " 'submitted',\n",
              " 'PDF',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------Stemming------------"
      ],
      "metadata": {
        "id": "efu0akJS9Tmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "words = word_tokenize(health_care)\n",
        "stemmed = [stemmer.stem(word) for word in words]\n",
        "stemmed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4cnTwJjgPAj",
        "outputId": "819f967f-ed1e-4303-fc51-8a7b5451a0e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['thi',\n",
              " 'activ',\n",
              " 'focus',\n",
              " 'on',\n",
              " 'appli',\n",
              " 'preprocess',\n",
              " 'techniqu',\n",
              " 'to',\n",
              " 'sensit',\n",
              " 'medic',\n",
              " 'or',\n",
              " 'healthcare-rel',\n",
              " 'text',\n",
              " 'use',\n",
              " 'natur',\n",
              " 'languag',\n",
              " 'process',\n",
              " 'tool',\n",
              " '.',\n",
              " 'a',\n",
              " 'simplifi',\n",
              " 'medic',\n",
              " 'text',\n",
              " 'corpu',\n",
              " ',',\n",
              " 'such',\n",
              " 'as',\n",
              " 'pubm',\n",
              " 'abstract',\n",
              " 'or',\n",
              " 'curat',\n",
              " 'health',\n",
              " 'articl',\n",
              " ',',\n",
              " 'is',\n",
              " 'first',\n",
              " 'load',\n",
              " 'into',\n",
              " 'a',\n",
              " 'googl',\n",
              " 'colab',\n",
              " 'environ',\n",
              " 'run',\n",
              " 'python',\n",
              " '3.x',\n",
              " 'with',\n",
              " 'the',\n",
              " 'requir',\n",
              " 'librari',\n",
              " 'nltk',\n",
              " 'and',\n",
              " 'spaci',\n",
              " 'instal',\n",
              " '.',\n",
              " 'the',\n",
              " 'text',\n",
              " 'is',\n",
              " 'then',\n",
              " 'preprocess',\n",
              " 'by',\n",
              " 'perform',\n",
              " 'sentenc',\n",
              " 'and',\n",
              " 'word',\n",
              " 'token',\n",
              " 'use',\n",
              " 'both',\n",
              " 'nltk',\n",
              " 'and',\n",
              " 'spaci',\n",
              " 'to',\n",
              " 'break',\n",
              " 'the',\n",
              " 'medic',\n",
              " 'content',\n",
              " 'into',\n",
              " 'structur',\n",
              " 'linguist',\n",
              " 'unit',\n",
              " '.',\n",
              " 'further',\n",
              " 'preprocess',\n",
              " 'includ',\n",
              " 'appli',\n",
              " 'stem',\n",
              " 'to',\n",
              " 'reduc',\n",
              " 'word',\n",
              " 'to',\n",
              " 'their',\n",
              " 'root',\n",
              " 'form',\n",
              " 'and',\n",
              " 'lemmat',\n",
              " 'to',\n",
              " 'convert',\n",
              " 'medic',\n",
              " 'term',\n",
              " 'into',\n",
              " 'their',\n",
              " 'meaning',\n",
              " 'dictionari',\n",
              " 'base',\n",
              " 'form',\n",
              " '.',\n",
              " 'the',\n",
              " 'output',\n",
              " 'of',\n",
              " 'stem',\n",
              " 'and',\n",
              " 'lemmat',\n",
              " 'are',\n",
              " 'compar',\n",
              " 'to',\n",
              " 'highlight',\n",
              " 'their',\n",
              " 'differ',\n",
              " ',',\n",
              " 'show',\n",
              " 'that',\n",
              " 'stem',\n",
              " 'may',\n",
              " 'distort',\n",
              " 'medic',\n",
              " 'term',\n",
              " 'while',\n",
              " 'lemmat',\n",
              " 'preserv',\n",
              " 'semant',\n",
              " 'and',\n",
              " 'clinic',\n",
              " 'mean',\n",
              " '.',\n",
              " 'thi',\n",
              " 'comparison',\n",
              " 'lead',\n",
              " 'to',\n",
              " 'a',\n",
              " 'discuss',\n",
              " 'emphas',\n",
              " 'whi',\n",
              " 'lemmat',\n",
              " 'is',\n",
              " 'critic',\n",
              " 'in',\n",
              " 'healthcar',\n",
              " 'nlp',\n",
              " ',',\n",
              " 'as',\n",
              " 'accur',\n",
              " 'terminolog',\n",
              " 'is',\n",
              " 'essenti',\n",
              " 'for',\n",
              " 'task',\n",
              " 'such',\n",
              " 'as',\n",
              " 'clinic',\n",
              " 'text',\n",
              " 'analysi',\n",
              " ',',\n",
              " 'inform',\n",
              " 'retriev',\n",
              " ',',\n",
              " 'medic',\n",
              " 'code',\n",
              " ',',\n",
              " 'and',\n",
              " 'decis',\n",
              " 'support',\n",
              " 'system',\n",
              " '.',\n",
              " 'the',\n",
              " 'complet',\n",
              " 'workflow',\n",
              " 'is',\n",
              " 'document',\n",
              " 'in',\n",
              " 'a',\n",
              " 'googl',\n",
              " 'colab',\n",
              " 'notebook',\n",
              " 'with',\n",
              " 'proper',\n",
              " 'head',\n",
              " 'and',\n",
              " 'a',\n",
              " 'discuss',\n",
              " 'section',\n",
              " 'at',\n",
              " 'the',\n",
              " 'end',\n",
              " ',',\n",
              " 'and',\n",
              " 'the',\n",
              " 'final',\n",
              " 'notebook',\n",
              " 'is',\n",
              " 'submit',\n",
              " 'as',\n",
              " 'a',\n",
              " 'pdf',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d42ee1e5",
        "outputId": "c9049363-0dec-419e-f732-b33eaaf53a22"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "health_care = \"\"\"This activity focuses on applying preprocessing techniques to sensitive medical or healthcare-related text using Natural Language Processing tools. A simplified medical text corpus, such as PubMed abstracts or curated health articles, is first loaded into a Google Colab environment running Python 3.x with the required libraries NLTK and spaCy installed. The text is then preprocessed by performing sentence and word tokenization using both NLTK and spaCy to break the medical content into structured linguistic units. Further preprocessing includes applying stemming to reduce words to their root forms and lemmatization to convert medical terms into their meaningful dictionary base forms. The outputs of stemming and lemmatization are compared to highlight their differences, showing that stemming may distort medical terms while lemmatization preserves semantic and clinical meaning. This comparison leads to a discussion emphasizing why lemmatization is critical in healthcare NLP, as accurate terminology is essential for tasks such as clinical text analysis, information retrieval, medical coding, and decision support systems. The complete workflow is documented in a Google Colab notebook with proper headings and a discussion section at the end, and the final notebook is submitted as a PDF.\"\"\"\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "snowball = SnowballStemmer('english')\n",
        "words = word_tokenize(health_care)\n",
        "for word in words:\n",
        "  print(word,\"---->\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This ---->\n",
            "activity ---->\n",
            "focuses ---->\n",
            "on ---->\n",
            "applying ---->\n",
            "preprocessing ---->\n",
            "techniques ---->\n",
            "to ---->\n",
            "sensitive ---->\n",
            "medical ---->\n",
            "or ---->\n",
            "healthcare-related ---->\n",
            "text ---->\n",
            "using ---->\n",
            "Natural ---->\n",
            "Language ---->\n",
            "Processing ---->\n",
            "tools ---->\n",
            ". ---->\n",
            "A ---->\n",
            "simplified ---->\n",
            "medical ---->\n",
            "text ---->\n",
            "corpus ---->\n",
            ", ---->\n",
            "such ---->\n",
            "as ---->\n",
            "PubMed ---->\n",
            "abstracts ---->\n",
            "or ---->\n",
            "curated ---->\n",
            "health ---->\n",
            "articles ---->\n",
            ", ---->\n",
            "is ---->\n",
            "first ---->\n",
            "loaded ---->\n",
            "into ---->\n",
            "a ---->\n",
            "Google ---->\n",
            "Colab ---->\n",
            "environment ---->\n",
            "running ---->\n",
            "Python ---->\n",
            "3.x ---->\n",
            "with ---->\n",
            "the ---->\n",
            "required ---->\n",
            "libraries ---->\n",
            "NLTK ---->\n",
            "and ---->\n",
            "spaCy ---->\n",
            "installed ---->\n",
            ". ---->\n",
            "The ---->\n",
            "text ---->\n",
            "is ---->\n",
            "then ---->\n",
            "preprocessed ---->\n",
            "by ---->\n",
            "performing ---->\n",
            "sentence ---->\n",
            "and ---->\n",
            "word ---->\n",
            "tokenization ---->\n",
            "using ---->\n",
            "both ---->\n",
            "NLTK ---->\n",
            "and ---->\n",
            "spaCy ---->\n",
            "to ---->\n",
            "break ---->\n",
            "the ---->\n",
            "medical ---->\n",
            "content ---->\n",
            "into ---->\n",
            "structured ---->\n",
            "linguistic ---->\n",
            "units ---->\n",
            ". ---->\n",
            "Further ---->\n",
            "preprocessing ---->\n",
            "includes ---->\n",
            "applying ---->\n",
            "stemming ---->\n",
            "to ---->\n",
            "reduce ---->\n",
            "words ---->\n",
            "to ---->\n",
            "their ---->\n",
            "root ---->\n",
            "forms ---->\n",
            "and ---->\n",
            "lemmatization ---->\n",
            "to ---->\n",
            "convert ---->\n",
            "medical ---->\n",
            "terms ---->\n",
            "into ---->\n",
            "their ---->\n",
            "meaningful ---->\n",
            "dictionary ---->\n",
            "base ---->\n",
            "forms ---->\n",
            ". ---->\n",
            "The ---->\n",
            "outputs ---->\n",
            "of ---->\n",
            "stemming ---->\n",
            "and ---->\n",
            "lemmatization ---->\n",
            "are ---->\n",
            "compared ---->\n",
            "to ---->\n",
            "highlight ---->\n",
            "their ---->\n",
            "differences ---->\n",
            ", ---->\n",
            "showing ---->\n",
            "that ---->\n",
            "stemming ---->\n",
            "may ---->\n",
            "distort ---->\n",
            "medical ---->\n",
            "terms ---->\n",
            "while ---->\n",
            "lemmatization ---->\n",
            "preserves ---->\n",
            "semantic ---->\n",
            "and ---->\n",
            "clinical ---->\n",
            "meaning ---->\n",
            ". ---->\n",
            "This ---->\n",
            "comparison ---->\n",
            "leads ---->\n",
            "to ---->\n",
            "a ---->\n",
            "discussion ---->\n",
            "emphasizing ---->\n",
            "why ---->\n",
            "lemmatization ---->\n",
            "is ---->\n",
            "critical ---->\n",
            "in ---->\n",
            "healthcare ---->\n",
            "NLP ---->\n",
            ", ---->\n",
            "as ---->\n",
            "accurate ---->\n",
            "terminology ---->\n",
            "is ---->\n",
            "essential ---->\n",
            "for ---->\n",
            "tasks ---->\n",
            "such ---->\n",
            "as ---->\n",
            "clinical ---->\n",
            "text ---->\n",
            "analysis ---->\n",
            ", ---->\n",
            "information ---->\n",
            "retrieval ---->\n",
            ", ---->\n",
            "medical ---->\n",
            "coding ---->\n",
            ", ---->\n",
            "and ---->\n",
            "decision ---->\n",
            "support ---->\n",
            "systems ---->\n",
            ". ---->\n",
            "The ---->\n",
            "complete ---->\n",
            "workflow ---->\n",
            "is ---->\n",
            "documented ---->\n",
            "in ---->\n",
            "a ---->\n",
            "Google ---->\n",
            "Colab ---->\n",
            "notebook ---->\n",
            "with ---->\n",
            "proper ---->\n",
            "headings ---->\n",
            "and ---->\n",
            "a ---->\n",
            "discussion ---->\n",
            "section ---->\n",
            "at ---->\n",
            "the ---->\n",
            "end ---->\n",
            ", ---->\n",
            "and ---->\n",
            "the ---->\n",
            "final ---->\n",
            "notebook ---->\n",
            "is ---->\n",
            "submitted ---->\n",
            "as ---->\n",
            "a ---->\n",
            "PDF ---->\n",
            ". ---->\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import LancasterStemmer\n",
        "lancaster = LancasterStemmer()\n",
        "words = word_tokenize(health_care)\n",
        "for word in words:\n",
        "  print(word,\"---->\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpFsPmEfikQp",
        "outputId": "b406af89-8c40-4767-a64a-98240299a807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This ---->\n",
            "activity ---->\n",
            "focuses ---->\n",
            "on ---->\n",
            "applying ---->\n",
            "preprocessing ---->\n",
            "techniques ---->\n",
            "to ---->\n",
            "sensitive ---->\n",
            "medical ---->\n",
            "or ---->\n",
            "healthcare-related ---->\n",
            "text ---->\n",
            "using ---->\n",
            "Natural ---->\n",
            "Language ---->\n",
            "Processing ---->\n",
            "tools ---->\n",
            ". ---->\n",
            "A ---->\n",
            "simplified ---->\n",
            "medical ---->\n",
            "text ---->\n",
            "corpus ---->\n",
            ", ---->\n",
            "such ---->\n",
            "as ---->\n",
            "PubMed ---->\n",
            "abstracts ---->\n",
            "or ---->\n",
            "curated ---->\n",
            "health ---->\n",
            "articles ---->\n",
            ", ---->\n",
            "is ---->\n",
            "first ---->\n",
            "loaded ---->\n",
            "into ---->\n",
            "a ---->\n",
            "Google ---->\n",
            "Colab ---->\n",
            "environment ---->\n",
            "running ---->\n",
            "Python ---->\n",
            "3.x ---->\n",
            "with ---->\n",
            "the ---->\n",
            "required ---->\n",
            "libraries ---->\n",
            "NLTK ---->\n",
            "and ---->\n",
            "spaCy ---->\n",
            "installed ---->\n",
            ". ---->\n",
            "The ---->\n",
            "text ---->\n",
            "is ---->\n",
            "then ---->\n",
            "preprocessed ---->\n",
            "by ---->\n",
            "performing ---->\n",
            "sentence ---->\n",
            "and ---->\n",
            "word ---->\n",
            "tokenization ---->\n",
            "using ---->\n",
            "both ---->\n",
            "NLTK ---->\n",
            "and ---->\n",
            "spaCy ---->\n",
            "to ---->\n",
            "break ---->\n",
            "the ---->\n",
            "medical ---->\n",
            "content ---->\n",
            "into ---->\n",
            "structured ---->\n",
            "linguistic ---->\n",
            "units ---->\n",
            ". ---->\n",
            "Further ---->\n",
            "preprocessing ---->\n",
            "includes ---->\n",
            "applying ---->\n",
            "stemming ---->\n",
            "to ---->\n",
            "reduce ---->\n",
            "words ---->\n",
            "to ---->\n",
            "their ---->\n",
            "root ---->\n",
            "forms ---->\n",
            "and ---->\n",
            "lemmatization ---->\n",
            "to ---->\n",
            "convert ---->\n",
            "medical ---->\n",
            "terms ---->\n",
            "into ---->\n",
            "their ---->\n",
            "meaningful ---->\n",
            "dictionary ---->\n",
            "base ---->\n",
            "forms ---->\n",
            ". ---->\n",
            "The ---->\n",
            "outputs ---->\n",
            "of ---->\n",
            "stemming ---->\n",
            "and ---->\n",
            "lemmatization ---->\n",
            "are ---->\n",
            "compared ---->\n",
            "to ---->\n",
            "highlight ---->\n",
            "their ---->\n",
            "differences ---->\n",
            ", ---->\n",
            "showing ---->\n",
            "that ---->\n",
            "stemming ---->\n",
            "may ---->\n",
            "distort ---->\n",
            "medical ---->\n",
            "terms ---->\n",
            "while ---->\n",
            "lemmatization ---->\n",
            "preserves ---->\n",
            "semantic ---->\n",
            "and ---->\n",
            "clinical ---->\n",
            "meaning ---->\n",
            ". ---->\n",
            "This ---->\n",
            "comparison ---->\n",
            "leads ---->\n",
            "to ---->\n",
            "a ---->\n",
            "discussion ---->\n",
            "emphasizing ---->\n",
            "why ---->\n",
            "lemmatization ---->\n",
            "is ---->\n",
            "critical ---->\n",
            "in ---->\n",
            "healthcare ---->\n",
            "NLP ---->\n",
            ", ---->\n",
            "as ---->\n",
            "accurate ---->\n",
            "terminology ---->\n",
            "is ---->\n",
            "essential ---->\n",
            "for ---->\n",
            "tasks ---->\n",
            "such ---->\n",
            "as ---->\n",
            "clinical ---->\n",
            "text ---->\n",
            "analysis ---->\n",
            ", ---->\n",
            "information ---->\n",
            "retrieval ---->\n",
            ", ---->\n",
            "medical ---->\n",
            "coding ---->\n",
            ", ---->\n",
            "and ---->\n",
            "decision ---->\n",
            "support ---->\n",
            "systems ---->\n",
            ". ---->\n",
            "The ---->\n",
            "complete ---->\n",
            "workflow ---->\n",
            "is ---->\n",
            "documented ---->\n",
            "in ---->\n",
            "a ---->\n",
            "Google ---->\n",
            "Colab ---->\n",
            "notebook ---->\n",
            "with ---->\n",
            "proper ---->\n",
            "headings ---->\n",
            "and ---->\n",
            "a ---->\n",
            "discussion ---->\n",
            "section ---->\n",
            "at ---->\n",
            "the ---->\n",
            "end ---->\n",
            ", ---->\n",
            "and ---->\n",
            "the ---->\n",
            "final ---->\n",
            "notebook ---->\n",
            "is ---->\n",
            "submitted ---->\n",
            "as ---->\n",
            "a ---->\n",
            "PDF ---->\n",
            ". ---->\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "words = word_tokenize(health_care)\n",
        "for word in words:\n",
        "  print(word,\"---->\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3NeDlg2i5DK",
        "outputId": "d5fc2f40-fa02-4ca6-e9f5-50d026c6872b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This ---->\n",
            "activity ---->\n",
            "focuses ---->\n",
            "on ---->\n",
            "applying ---->\n",
            "preprocessing ---->\n",
            "techniques ---->\n",
            "to ---->\n",
            "sensitive ---->\n",
            "medical ---->\n",
            "or ---->\n",
            "healthcare-related ---->\n",
            "text ---->\n",
            "using ---->\n",
            "Natural ---->\n",
            "Language ---->\n",
            "Processing ---->\n",
            "tools ---->\n",
            ". ---->\n",
            "A ---->\n",
            "simplified ---->\n",
            "medical ---->\n",
            "text ---->\n",
            "corpus ---->\n",
            ", ---->\n",
            "such ---->\n",
            "as ---->\n",
            "PubMed ---->\n",
            "abstracts ---->\n",
            "or ---->\n",
            "curated ---->\n",
            "health ---->\n",
            "articles ---->\n",
            ", ---->\n",
            "is ---->\n",
            "first ---->\n",
            "loaded ---->\n",
            "into ---->\n",
            "a ---->\n",
            "Google ---->\n",
            "Colab ---->\n",
            "environment ---->\n",
            "running ---->\n",
            "Python ---->\n",
            "3.x ---->\n",
            "with ---->\n",
            "the ---->\n",
            "required ---->\n",
            "libraries ---->\n",
            "NLTK ---->\n",
            "and ---->\n",
            "spaCy ---->\n",
            "installed ---->\n",
            ". ---->\n",
            "The ---->\n",
            "text ---->\n",
            "is ---->\n",
            "then ---->\n",
            "preprocessed ---->\n",
            "by ---->\n",
            "performing ---->\n",
            "sentence ---->\n",
            "and ---->\n",
            "word ---->\n",
            "tokenization ---->\n",
            "using ---->\n",
            "both ---->\n",
            "NLTK ---->\n",
            "and ---->\n",
            "spaCy ---->\n",
            "to ---->\n",
            "break ---->\n",
            "the ---->\n",
            "medical ---->\n",
            "content ---->\n",
            "into ---->\n",
            "structured ---->\n",
            "linguistic ---->\n",
            "units ---->\n",
            ". ---->\n",
            "Further ---->\n",
            "preprocessing ---->\n",
            "includes ---->\n",
            "applying ---->\n",
            "stemming ---->\n",
            "to ---->\n",
            "reduce ---->\n",
            "words ---->\n",
            "to ---->\n",
            "their ---->\n",
            "root ---->\n",
            "forms ---->\n",
            "and ---->\n",
            "lemmatization ---->\n",
            "to ---->\n",
            "convert ---->\n",
            "medical ---->\n",
            "terms ---->\n",
            "into ---->\n",
            "their ---->\n",
            "meaningful ---->\n",
            "dictionary ---->\n",
            "base ---->\n",
            "forms ---->\n",
            ". ---->\n",
            "The ---->\n",
            "outputs ---->\n",
            "of ---->\n",
            "stemming ---->\n",
            "and ---->\n",
            "lemmatization ---->\n",
            "are ---->\n",
            "compared ---->\n",
            "to ---->\n",
            "highlight ---->\n",
            "their ---->\n",
            "differences ---->\n",
            ", ---->\n",
            "showing ---->\n",
            "that ---->\n",
            "stemming ---->\n",
            "may ---->\n",
            "distort ---->\n",
            "medical ---->\n",
            "terms ---->\n",
            "while ---->\n",
            "lemmatization ---->\n",
            "preserves ---->\n",
            "semantic ---->\n",
            "and ---->\n",
            "clinical ---->\n",
            "meaning ---->\n",
            ". ---->\n",
            "This ---->\n",
            "comparison ---->\n",
            "leads ---->\n",
            "to ---->\n",
            "a ---->\n",
            "discussion ---->\n",
            "emphasizing ---->\n",
            "why ---->\n",
            "lemmatization ---->\n",
            "is ---->\n",
            "critical ---->\n",
            "in ---->\n",
            "healthcare ---->\n",
            "NLP ---->\n",
            ", ---->\n",
            "as ---->\n",
            "accurate ---->\n",
            "terminology ---->\n",
            "is ---->\n",
            "essential ---->\n",
            "for ---->\n",
            "tasks ---->\n",
            "such ---->\n",
            "as ---->\n",
            "clinical ---->\n",
            "text ---->\n",
            "analysis ---->\n",
            ", ---->\n",
            "information ---->\n",
            "retrieval ---->\n",
            ", ---->\n",
            "medical ---->\n",
            "coding ---->\n",
            ", ---->\n",
            "and ---->\n",
            "decision ---->\n",
            "support ---->\n",
            "systems ---->\n",
            ". ---->\n",
            "The ---->\n",
            "complete ---->\n",
            "workflow ---->\n",
            "is ---->\n",
            "documented ---->\n",
            "in ---->\n",
            "a ---->\n",
            "Google ---->\n",
            "Colab ---->\n",
            "notebook ---->\n",
            "with ---->\n",
            "proper ---->\n",
            "headings ---->\n",
            "and ---->\n",
            "a ---->\n",
            "discussion ---->\n",
            "section ---->\n",
            "at ---->\n",
            "the ---->\n",
            "end ---->\n",
            ", ---->\n",
            "and ---->\n",
            "the ---->\n",
            "final ---->\n",
            "notebook ---->\n",
            "is ---->\n",
            "submitted ---->\n",
            "as ---->\n",
            "a ---->\n",
            "PDF ---->\n",
            ". ---->\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------Lemmatization---------------"
      ],
      "metadata": {
        "id": "GyVWJ0aX9mJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health_care = \"\"\"This activity focuses on applying preprocessing techniques to sensitive medical or healthcare-related text using Natural Language Processing tools. A simplified medical text corpus, such as PubMed abstracts or curated health articles, is first loaded into a Google Colab environment running Python 3.x with the required libraries NLTK and spaCy installed. The text is then preprocessed by performing sentence and word tokenization using both NLTK and spaCy to break the medical content into structured linguistic units. Further preprocessing includes applying stemming to reduce words to their root forms and lemmatization to convert medical terms into their meaningful dictionary base forms. The outputs of stemming and lemmatization are compared to highlight their differences, showing that stemming may distort medical terms while lemmatization preserves semantic and clinical meaning. This comparison leads to a discussion emphasizing why lemmatization is critical in healthcare NLP, as accurate terminology is essential for tasks such as clinical text analysis, information retrieval, medical coding, and decision support systems. The complete workflow is documented in a Google Colab notebook with proper headings and a discussion section at the end, and the final notebook is submitted as a PDF.\"\"\"\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = word_tokenize(health_care)\n",
        "for word in words:\n",
        "  print(lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dh135Kc3jMCk",
        "outputId": "4502fbfc-5e59-4562-e87a-8046d49e1adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This\n",
            "activity\n",
            "focus\n",
            "on\n",
            "applying\n",
            "preprocessing\n",
            "technique\n",
            "to\n",
            "sensitive\n",
            "medical\n",
            "or\n",
            "healthcare-related\n",
            "text\n",
            "using\n",
            "Natural\n",
            "Language\n",
            "Processing\n",
            "tool\n",
            ".\n",
            "A\n",
            "simplified\n",
            "medical\n",
            "text\n",
            "corpus\n",
            ",\n",
            "such\n",
            "a\n",
            "PubMed\n",
            "abstract\n",
            "or\n",
            "curated\n",
            "health\n",
            "article\n",
            ",\n",
            "is\n",
            "first\n",
            "loaded\n",
            "into\n",
            "a\n",
            "Google\n",
            "Colab\n",
            "environment\n",
            "running\n",
            "Python\n",
            "3.x\n",
            "with\n",
            "the\n",
            "required\n",
            "library\n",
            "NLTK\n",
            "and\n",
            "spaCy\n",
            "installed\n",
            ".\n",
            "The\n",
            "text\n",
            "is\n",
            "then\n",
            "preprocessed\n",
            "by\n",
            "performing\n",
            "sentence\n",
            "and\n",
            "word\n",
            "tokenization\n",
            "using\n",
            "both\n",
            "NLTK\n",
            "and\n",
            "spaCy\n",
            "to\n",
            "break\n",
            "the\n",
            "medical\n",
            "content\n",
            "into\n",
            "structured\n",
            "linguistic\n",
            "unit\n",
            ".\n",
            "Further\n",
            "preprocessing\n",
            "includes\n",
            "applying\n",
            "stemming\n",
            "to\n",
            "reduce\n",
            "word\n",
            "to\n",
            "their\n",
            "root\n",
            "form\n",
            "and\n",
            "lemmatization\n",
            "to\n",
            "convert\n",
            "medical\n",
            "term\n",
            "into\n",
            "their\n",
            "meaningful\n",
            "dictionary\n",
            "base\n",
            "form\n",
            ".\n",
            "The\n",
            "output\n",
            "of\n",
            "stemming\n",
            "and\n",
            "lemmatization\n",
            "are\n",
            "compared\n",
            "to\n",
            "highlight\n",
            "their\n",
            "difference\n",
            ",\n",
            "showing\n",
            "that\n",
            "stemming\n",
            "may\n",
            "distort\n",
            "medical\n",
            "term\n",
            "while\n",
            "lemmatization\n",
            "preserve\n",
            "semantic\n",
            "and\n",
            "clinical\n",
            "meaning\n",
            ".\n",
            "This\n",
            "comparison\n",
            "lead\n",
            "to\n",
            "a\n",
            "discussion\n",
            "emphasizing\n",
            "why\n",
            "lemmatization\n",
            "is\n",
            "critical\n",
            "in\n",
            "healthcare\n",
            "NLP\n",
            ",\n",
            "a\n",
            "accurate\n",
            "terminology\n",
            "is\n",
            "essential\n",
            "for\n",
            "task\n",
            "such\n",
            "a\n",
            "clinical\n",
            "text\n",
            "analysis\n",
            ",\n",
            "information\n",
            "retrieval\n",
            ",\n",
            "medical\n",
            "coding\n",
            ",\n",
            "and\n",
            "decision\n",
            "support\n",
            "system\n",
            ".\n",
            "The\n",
            "complete\n",
            "workflow\n",
            "is\n",
            "documented\n",
            "in\n",
            "a\n",
            "Google\n",
            "Colab\n",
            "notebook\n",
            "with\n",
            "proper\n",
            "heading\n",
            "and\n",
            "a\n",
            "discussion\n",
            "section\n",
            "at\n",
            "the\n",
            "end\n",
            ",\n",
            "and\n",
            "the\n",
            "final\n",
            "notebook\n",
            "is\n",
            "submitted\n",
            "a\n",
            "a\n",
            "PDF\n",
            ".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"better\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "W_dW_JsZkUo0",
        "outputId": "1b1d7f76-6e35-4898-8fdd-9386c18d7236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'better'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"better\", pos=\"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "L_AUCtMykcUG",
        "outputId": "4cc1dc6b-32e6-4abe-cc4c-6c5398d7a979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------Comaparision--------------\n"
      ],
      "metadata": {
        "id": "4qBoSJUF9uSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer,SnowballStemmer,LancasterStemmer,RegexpStemmer,WordNetLemmatizer\n",
        "poter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer('english')\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\"]\n",
        "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}{5:50}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",\"Regexp Stemmer\",\"WordNet Lemmatizer\"))\n",
        "print(\"-\"*160)\n",
        "for word in word_list:\n",
        "      print(\"{0:20}{1:20}{2:20}{3:30}{4:40}{5:50}\".format(word, poter.stem(word), snowball.stem(word), lancaster.stem(word), regexp.stem(word), lemmatizer.lemmatize(word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ays2Ctfwke4c",
        "outputId": "9a1cb6b0-74f9-4acc-e290-834508a4453f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word                Porter Stemmer      Snowball Stemmer    Lancaster Stemmer             Regexp Stemmer                          WordNet Lemmatizer                                \n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "friend              friend              friend              friend                        friend                                  friend                                            \n",
            "friendship          friendship          friendship          friend                        friendship                              friendship                                        \n",
            "friends             friend              friend              friend                        friend                                  friend                                            \n",
            "friendships         friendship          friendship          friend                        friendship                              friendship                                        \n",
            "stabil              stabil              stabil              stabl                         stabil                                  stabil                                            \n",
            "destabilize         destabil            destabil            dest                          destabiliz                              destabilize                                       \n",
            "misunderstanding    misunderstand       misunderstand       misunderstand                 misunderstand                           misunderstanding                                  \n",
            "railroad            railroad            railroad            railroad                      railroad                                railroad                                          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------PPT ASSIGNMENT-----------------"
      ],
      "metadata": {
        "id": "d6AVWKHc91nP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "write preprocessing output for :\n",
        "\n",
        "# \"NLP models are transforming the world rapidly!"
      ],
      "metadata": {
        "id": "TRGl4gTx96TZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy"
      ],
      "metadata": {
        "id": "RoIJTJKL-A2b"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "subject=\"NLP models are transforming the world rapidly!\"\n"
      ],
      "metadata": {
        "id": "uPmRZ5XgAOyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "subject=\"NLP models are transforming the world rapidly!\"\n",
        "word_tokenize(subject)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-Dy0Ww6ASa2",
        "outputId": "dfc5770a-2260-4a94-df62-afaf90a9c3e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'models', 'are', 'transforming', 'the', 'world', 'rapidly', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(subject)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuLtazRRAjYn",
        "outputId": "cc1086d6-9749-4827-f0d4-708b44f977d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP models are transforming the world rapidly!']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA1u7cBtApDy",
        "outputId": "05af129e-ca9a-47d1-dc37-47217cd86491"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_quote = word_tokenize(subject)\n",
        "words_in_quote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnhj-4DcAvq9",
        "outputId": "841756c2-1fc9-4243-8bdf-d3a53cb24cbb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'models', 'are', 'transforming', 'the', 'world', 'rapidly', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "subject = \"NLP models are transforming the world rapidly!\"\n"
      ],
      "metadata": {
        "id": "ffO-B3uEA343"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "words_in_quote = word_tokenize(subject)\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_list = []\n",
        "for word in words_in_quote:\n",
        "  if word.casefold() not in stop_words:\n",
        "        filtered_list.append(word)\n",
        "filtered_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7m2_I_j1A-94",
        "outputId": "c1c2adb5-f4fc-4663-f80f-c1f702d84a81"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'models', 'transforming', 'world', 'rapidly', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "regexp_stemmer = RegexpStemmer(r'ing$|e$', min=4)\n",
        "words = word_tokenize(subject)\n",
        "stemmed_words = [regexp_stemmer.stem(word) for word in words]\n",
        "stemmed_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RZqOnZ_BNw0",
        "outputId": "aa02c4cf-382e-4b2c-8b69-487ac9125406"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'models', 'are', 'transform', 'the', 'world', 'rapidly', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "subject = \"NLP models are transforming the world rapidly!\"\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "snowball = SnowballStemmer(language='english')\n",
        "words = word_tokenize(subject)\n",
        "for word in words:\n",
        "    print(word,\"---->\",snowball.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_KrrWmBBnlT",
        "outputId": "99928b18-e753-4012-a08e-8e127009452d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP ----> nlp\n",
            "models ----> model\n",
            "are ----> are\n",
            "transforming ----> transform\n",
            "the ----> the\n",
            "world ----> world\n",
            "rapidly ----> rapid\n",
            "! ----> !\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import LancasterStemmer\n",
        "Lanc = LancasterStemmer()\n",
        "words = word_tokenize(subject)\n",
        "for word in words:\n",
        "  print(word,\"---->\",Lanc.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSzitOe0BsXn",
        "outputId": "19d34466-a4ec-4b38-a4f4-9d1ffd9fc06e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP ----> nlp\n",
            "models ----> model\n",
            "are ----> ar\n",
            "transforming ----> transform\n",
            "the ----> the\n",
            "world ----> world\n",
            "rapidly ----> rapid\n",
            "! ----> !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "regexp = RegexpStemmer('ing|e', min=4)\n",
        "words = word_tokenize(subject)\n",
        "for word in words:\n",
        "  print(word,\"---->\",regexp.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRhNs25IBvz5",
        "outputId": "a0a2a93a-867d-4964-ca77-9f7e0060310c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP ----> NLP\n",
            "models ----> modls\n",
            "are ----> are\n",
            "transforming ----> transform\n",
            "the ----> the\n",
            "world ----> world\n",
            "rapidly ----> rapidly\n",
            "! ----> !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "subject = \"NLP models are transforming the world rapidly!\"\n"
      ],
      "metadata": {
        "id": "18TyjMU9CHES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = word_tokenize(subject)\n",
        "for word in words:\n",
        "  print(word,\"---->\",lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4afmp4lCGGg",
        "outputId": "5c6c378c-3f06-4a43-cfb1-d3abe6146b65"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP ----> NLP\n",
            "models ----> model\n",
            "are ----> are\n",
            "transforming ----> transforming\n",
            "the ----> the\n",
            "world ----> world\n",
            "rapidly ----> rapidly\n",
            "! ----> !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"better\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0xjC-QO6CchA",
        "outputId": "a9e3b695-ccc9-4a3a-9981-050e9ed22317"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'better'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"better\", pos=\"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jLsaZM1qCfDZ",
        "outputId": "c8070d98-41a6-486e-f0f0-f4f954bad53b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Git Hub **Assignment**"
      ],
      "metadata": {
        "id": "R-zOFGWPCjhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy"
      ],
      "metadata": {
        "id": "p3ofPPr8Cmpo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NIT=\"NIT College, also known as the National Institute of Technology, is one of the most prestigious engineering institutions in India. It is known for its high academic standards, experienced faculty, and excellent infrastructure. NIT colleges offer quality education in engineering, science, and technology, helping students develop strong technical and practical skills. With modern laboratories, libraries, and research facilities, NIT provides a good learning environment. Students from different parts of the country study together, promoting unity and cultural diversity. Overall, NIT College plays an important role in shaping skilled professionals and future leaders of the nation.\"\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(NIT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKp0aK98DmhV",
        "outputId": "f4fcc983-be08-4e44-a38d-04f017bdac60"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NIT',\n",
              " 'College',\n",
              " ',',\n",
              " 'also',\n",
              " 'known',\n",
              " 'as',\n",
              " 'the',\n",
              " 'National',\n",
              " 'Institute',\n",
              " 'of',\n",
              " 'Technology',\n",
              " ',',\n",
              " 'is',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'most',\n",
              " 'prestigious',\n",
              " 'engineering',\n",
              " 'institutions',\n",
              " 'in',\n",
              " 'India',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'known',\n",
              " 'for',\n",
              " 'its',\n",
              " 'high',\n",
              " 'academic',\n",
              " 'standards',\n",
              " ',',\n",
              " 'experienced',\n",
              " 'faculty',\n",
              " ',',\n",
              " 'and',\n",
              " 'excellent',\n",
              " 'infrastructure',\n",
              " '.',\n",
              " 'NIT',\n",
              " 'colleges',\n",
              " 'offer',\n",
              " 'quality',\n",
              " 'education',\n",
              " 'in',\n",
              " 'engineering',\n",
              " ',',\n",
              " 'science',\n",
              " ',',\n",
              " 'and',\n",
              " 'technology',\n",
              " ',',\n",
              " 'helping',\n",
              " 'students',\n",
              " 'develop',\n",
              " 'strong',\n",
              " 'technical',\n",
              " 'and',\n",
              " 'practical',\n",
              " 'skills',\n",
              " '.',\n",
              " 'With',\n",
              " 'modern',\n",
              " 'laboratories',\n",
              " ',',\n",
              " 'libraries',\n",
              " ',',\n",
              " 'and',\n",
              " 'research',\n",
              " 'facilities',\n",
              " ',',\n",
              " 'NIT',\n",
              " 'provides',\n",
              " 'a',\n",
              " 'good',\n",
              " 'learning',\n",
              " 'environment',\n",
              " '.',\n",
              " 'Students',\n",
              " 'from',\n",
              " 'different',\n",
              " 'parts',\n",
              " 'of',\n",
              " 'the',\n",
              " 'country',\n",
              " 'study',\n",
              " 'together',\n",
              " ',',\n",
              " 'promoting',\n",
              " 'unity',\n",
              " 'and',\n",
              " 'cultural',\n",
              " 'diversity',\n",
              " '.',\n",
              " 'Overall',\n",
              " ',',\n",
              " 'NIT',\n",
              " 'College',\n",
              " 'plays',\n",
              " 'an',\n",
              " 'important',\n",
              " 'role',\n",
              " 'in',\n",
              " 'shaping',\n",
              " 'skilled',\n",
              " 'professionals',\n",
              " 'and',\n",
              " 'future',\n",
              " 'leaders',\n",
              " 'of',\n",
              " 'the',\n",
              " 'nation',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(NIT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNCRY2AyD6ji",
        "outputId": "811c2840-92bd-458a-eb85-a0c911bfd03b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NIT College, also known as the National Institute of Technology, is one of the most prestigious engineering institutions in India.',\n",
              " 'It is known for its high academic standards, experienced faculty, and excellent infrastructure.',\n",
              " 'NIT colleges offer quality education in engineering, science, and technology, helping students develop strong technical and practical skills.',\n",
              " 'With modern laboratories, libraries, and research facilities, NIT provides a good learning environment.',\n",
              " 'Students from different parts of the country study together, promoting unity and cultural diversity.',\n",
              " 'Overall, NIT College plays an important role in shaping skilled professionals and future leaders of the nation.']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8zLeOXlD_Hj",
        "outputId": "c55c80be-1671-4a0e-c196-2286722f187e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_quote = word_tokenize(NIT)\n",
        "words_in_quote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7vR8agCEEW5",
        "outputId": "94a23f84-23c5-4f4b-acc4-687b4ccc7acb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NIT',\n",
              " 'College',\n",
              " ',',\n",
              " 'also',\n",
              " 'known',\n",
              " 'as',\n",
              " 'the',\n",
              " 'National',\n",
              " 'Institute',\n",
              " 'of',\n",
              " 'Technology',\n",
              " ',',\n",
              " 'is',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'most',\n",
              " 'prestigious',\n",
              " 'engineering',\n",
              " 'institutions',\n",
              " 'in',\n",
              " 'India',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'known',\n",
              " 'for',\n",
              " 'its',\n",
              " 'high',\n",
              " 'academic',\n",
              " 'standards',\n",
              " ',',\n",
              " 'experienced',\n",
              " 'faculty',\n",
              " ',',\n",
              " 'and',\n",
              " 'excellent',\n",
              " 'infrastructure',\n",
              " '.',\n",
              " 'NIT',\n",
              " 'colleges',\n",
              " 'offer',\n",
              " 'quality',\n",
              " 'education',\n",
              " 'in',\n",
              " 'engineering',\n",
              " ',',\n",
              " 'science',\n",
              " ',',\n",
              " 'and',\n",
              " 'technology',\n",
              " ',',\n",
              " 'helping',\n",
              " 'students',\n",
              " 'develop',\n",
              " 'strong',\n",
              " 'technical',\n",
              " 'and',\n",
              " 'practical',\n",
              " 'skills',\n",
              " '.',\n",
              " 'With',\n",
              " 'modern',\n",
              " 'laboratories',\n",
              " ',',\n",
              " 'libraries',\n",
              " ',',\n",
              " 'and',\n",
              " 'research',\n",
              " 'facilities',\n",
              " ',',\n",
              " 'NIT',\n",
              " 'provides',\n",
              " 'a',\n",
              " 'good',\n",
              " 'learning',\n",
              " 'environment',\n",
              " '.',\n",
              " 'Students',\n",
              " 'from',\n",
              " 'different',\n",
              " 'parts',\n",
              " 'of',\n",
              " 'the',\n",
              " 'country',\n",
              " 'study',\n",
              " 'together',\n",
              " ',',\n",
              " 'promoting',\n",
              " 'unity',\n",
              " 'and',\n",
              " 'cultural',\n",
              " 'diversity',\n",
              " '.',\n",
              " 'Overall',\n",
              " ',',\n",
              " 'NIT',\n",
              " 'College',\n",
              " 'plays',\n",
              " 'an',\n",
              " 'important',\n",
              " 'role',\n",
              " 'in',\n",
              " 'shaping',\n",
              " 'skilled',\n",
              " 'professionals',\n",
              " 'and',\n",
              " 'future',\n",
              " 'leaders',\n",
              " 'of',\n",
              " 'the',\n",
              " 'nation',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_list = []\n",
        "for word in words_in_quote:\n",
        "  if word.casefold() not in stop_words:\n",
        "    filtered_list.append(word)\n",
        "filtered_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKA9ph84ELdP",
        "outputId": "41cbb933-47fb-4e47-9551-c389659b137f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NIT',\n",
              " 'College',\n",
              " ',',\n",
              " 'also',\n",
              " 'known',\n",
              " 'National',\n",
              " 'Institute',\n",
              " 'Technology',\n",
              " ',',\n",
              " 'one',\n",
              " 'prestigious',\n",
              " 'engineering',\n",
              " 'institutions',\n",
              " 'India',\n",
              " '.',\n",
              " 'known',\n",
              " 'high',\n",
              " 'academic',\n",
              " 'standards',\n",
              " ',',\n",
              " 'experienced',\n",
              " 'faculty',\n",
              " ',',\n",
              " 'excellent',\n",
              " 'infrastructure',\n",
              " '.',\n",
              " 'NIT',\n",
              " 'colleges',\n",
              " 'offer',\n",
              " 'quality',\n",
              " 'education',\n",
              " 'engineering',\n",
              " ',',\n",
              " 'science',\n",
              " ',',\n",
              " 'technology',\n",
              " ',',\n",
              " 'helping',\n",
              " 'students',\n",
              " 'develop',\n",
              " 'strong',\n",
              " 'technical',\n",
              " 'practical',\n",
              " 'skills',\n",
              " '.',\n",
              " 'modern',\n",
              " 'laboratories',\n",
              " ',',\n",
              " 'libraries',\n",
              " ',',\n",
              " 'research',\n",
              " 'facilities',\n",
              " ',',\n",
              " 'NIT',\n",
              " 'provides',\n",
              " 'good',\n",
              " 'learning',\n",
              " 'environment',\n",
              " '.',\n",
              " 'Students',\n",
              " 'different',\n",
              " 'parts',\n",
              " 'country',\n",
              " 'study',\n",
              " 'together',\n",
              " ',',\n",
              " 'promoting',\n",
              " 'unity',\n",
              " 'cultural',\n",
              " 'diversity',\n",
              " '.',\n",
              " 'Overall',\n",
              " ',',\n",
              " 'NIT',\n",
              " 'College',\n",
              " 'plays',\n",
              " 'important',\n",
              " 'role',\n",
              " 'shaping',\n",
              " 'skilled',\n",
              " 'professionals',\n",
              " 'future',\n",
              " 'leaders',\n",
              " 'nation',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "words = word_tokenize(NIT)\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "stemmed_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGsWpNNeEQY6",
        "outputId": "c43f481b-b29f-4439-dc33-65959011054a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nit',\n",
              " 'colleg',\n",
              " ',',\n",
              " 'also',\n",
              " 'known',\n",
              " 'as',\n",
              " 'the',\n",
              " 'nation',\n",
              " 'institut',\n",
              " 'of',\n",
              " 'technolog',\n",
              " ',',\n",
              " 'is',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'most',\n",
              " 'prestigi',\n",
              " 'engin',\n",
              " 'institut',\n",
              " 'in',\n",
              " 'india',\n",
              " '.',\n",
              " 'it',\n",
              " 'is',\n",
              " 'known',\n",
              " 'for',\n",
              " 'it',\n",
              " 'high',\n",
              " 'academ',\n",
              " 'standard',\n",
              " ',',\n",
              " 'experienc',\n",
              " 'faculti',\n",
              " ',',\n",
              " 'and',\n",
              " 'excel',\n",
              " 'infrastructur',\n",
              " '.',\n",
              " 'nit',\n",
              " 'colleg',\n",
              " 'offer',\n",
              " 'qualiti',\n",
              " 'educ',\n",
              " 'in',\n",
              " 'engin',\n",
              " ',',\n",
              " 'scienc',\n",
              " ',',\n",
              " 'and',\n",
              " 'technolog',\n",
              " ',',\n",
              " 'help',\n",
              " 'student',\n",
              " 'develop',\n",
              " 'strong',\n",
              " 'technic',\n",
              " 'and',\n",
              " 'practic',\n",
              " 'skill',\n",
              " '.',\n",
              " 'with',\n",
              " 'modern',\n",
              " 'laboratori',\n",
              " ',',\n",
              " 'librari',\n",
              " ',',\n",
              " 'and',\n",
              " 'research',\n",
              " 'facil',\n",
              " ',',\n",
              " 'nit',\n",
              " 'provid',\n",
              " 'a',\n",
              " 'good',\n",
              " 'learn',\n",
              " 'environ',\n",
              " '.',\n",
              " 'student',\n",
              " 'from',\n",
              " 'differ',\n",
              " 'part',\n",
              " 'of',\n",
              " 'the',\n",
              " 'countri',\n",
              " 'studi',\n",
              " 'togeth',\n",
              " ',',\n",
              " 'promot',\n",
              " 'uniti',\n",
              " 'and',\n",
              " 'cultur',\n",
              " 'divers',\n",
              " '.',\n",
              " 'overal',\n",
              " ',',\n",
              " 'nit',\n",
              " 'colleg',\n",
              " 'play',\n",
              " 'an',\n",
              " 'import',\n",
              " 'role',\n",
              " 'in',\n",
              " 'shape',\n",
              " 'skill',\n",
              " 'profession',\n",
              " 'and',\n",
              " 'futur',\n",
              " 'leader',\n",
              " 'of',\n",
              " 'the',\n",
              " 'nation',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball = SnowballStemmer(language='english')\n",
        "words = word_tokenize(NIT)\n",
        "for word in words:\n",
        "    print(word,\"--->\",snowball.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBD0XIOwEZuz",
        "outputId": "137223bf-cf0b-44f9-caa4-8dcb2b80fec4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NIT ---> nit\n",
            "College ---> colleg\n",
            ", ---> ,\n",
            "also ---> also\n",
            "known ---> known\n",
            "as ---> as\n",
            "the ---> the\n",
            "National ---> nation\n",
            "Institute ---> institut\n",
            "of ---> of\n",
            "Technology ---> technolog\n",
            ", ---> ,\n",
            "is ---> is\n",
            "one ---> one\n",
            "of ---> of\n",
            "the ---> the\n",
            "most ---> most\n",
            "prestigious ---> prestigi\n",
            "engineering ---> engin\n",
            "institutions ---> institut\n",
            "in ---> in\n",
            "India ---> india\n",
            ". ---> .\n",
            "It ---> it\n",
            "is ---> is\n",
            "known ---> known\n",
            "for ---> for\n",
            "its ---> it\n",
            "high ---> high\n",
            "academic ---> academ\n",
            "standards ---> standard\n",
            ", ---> ,\n",
            "experienced ---> experienc\n",
            "faculty ---> faculti\n",
            ", ---> ,\n",
            "and ---> and\n",
            "excellent ---> excel\n",
            "infrastructure ---> infrastructur\n",
            ". ---> .\n",
            "NIT ---> nit\n",
            "colleges ---> colleg\n",
            "offer ---> offer\n",
            "quality ---> qualiti\n",
            "education ---> educ\n",
            "in ---> in\n",
            "engineering ---> engin\n",
            ", ---> ,\n",
            "science ---> scienc\n",
            ", ---> ,\n",
            "and ---> and\n",
            "technology ---> technolog\n",
            ", ---> ,\n",
            "helping ---> help\n",
            "students ---> student\n",
            "develop ---> develop\n",
            "strong ---> strong\n",
            "technical ---> technic\n",
            "and ---> and\n",
            "practical ---> practic\n",
            "skills ---> skill\n",
            ". ---> .\n",
            "With ---> with\n",
            "modern ---> modern\n",
            "laboratories ---> laboratori\n",
            ", ---> ,\n",
            "libraries ---> librari\n",
            ", ---> ,\n",
            "and ---> and\n",
            "research ---> research\n",
            "facilities ---> facil\n",
            ", ---> ,\n",
            "NIT ---> nit\n",
            "provides ---> provid\n",
            "a ---> a\n",
            "good ---> good\n",
            "learning ---> learn\n",
            "environment ---> environ\n",
            ". ---> .\n",
            "Students ---> student\n",
            "from ---> from\n",
            "different ---> differ\n",
            "parts ---> part\n",
            "of ---> of\n",
            "the ---> the\n",
            "country ---> countri\n",
            "study ---> studi\n",
            "together ---> togeth\n",
            ", ---> ,\n",
            "promoting ---> promot\n",
            "unity ---> uniti\n",
            "and ---> and\n",
            "cultural ---> cultur\n",
            "diversity ---> divers\n",
            ". ---> .\n",
            "Overall ---> overal\n",
            ", ---> ,\n",
            "NIT ---> nit\n",
            "College ---> colleg\n",
            "plays ---> play\n",
            "an ---> an\n",
            "important ---> import\n",
            "role ---> role\n",
            "in ---> in\n",
            "shaping ---> shape\n",
            "skilled ---> skill\n",
            "professionals ---> profession\n",
            "and ---> and\n",
            "future ---> futur\n",
            "leaders ---> leader\n",
            "of ---> of\n",
            "the ---> the\n",
            "nation ---> nation\n",
            ". ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import LancasterStemmer\n",
        "Lanc = LancasterStemmer()\n",
        "words = word_tokenize(NIT)\n",
        "for word in words:\n",
        "    print(word,\"--->\",Lanc.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkSMmkZKElLW",
        "outputId": "9cc06c8e-aa17-4b71-900d-a7e005c3056e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NIT ---> nit\n",
            "College ---> colleg\n",
            ", ---> ,\n",
            "also ---> also\n",
            "known ---> known\n",
            "as ---> as\n",
            "the ---> the\n",
            "National ---> nat\n",
            "Institute ---> institut\n",
            "of ---> of\n",
            "Technology ---> technolog\n",
            ", ---> ,\n",
            "is ---> is\n",
            "one ---> on\n",
            "of ---> of\n",
            "the ---> the\n",
            "most ---> most\n",
            "prestigious ---> prestigy\n",
            "engineering ---> engin\n",
            "institutions ---> institut\n",
            "in ---> in\n",
            "India ---> ind\n",
            ". ---> .\n",
            "It ---> it\n",
            "is ---> is\n",
            "known ---> known\n",
            "for ---> for\n",
            "its ---> it\n",
            "high ---> high\n",
            "academic ---> academ\n",
            "standards ---> standard\n",
            ", ---> ,\n",
            "experienced ---> expery\n",
            "faculty ---> facul\n",
            ", ---> ,\n",
            "and ---> and\n",
            "excellent ---> excel\n",
            "infrastructure ---> infrastruct\n",
            ". ---> .\n",
            "NIT ---> nit\n",
            "colleges ---> colleg\n",
            "offer ---> off\n",
            "quality ---> qual\n",
            "education ---> educ\n",
            "in ---> in\n",
            "engineering ---> engin\n",
            ", ---> ,\n",
            "science ---> sci\n",
            ", ---> ,\n",
            "and ---> and\n",
            "technology ---> technolog\n",
            ", ---> ,\n",
            "helping ---> help\n",
            "students ---> stud\n",
            "develop ---> develop\n",
            "strong ---> strong\n",
            "technical ---> techn\n",
            "and ---> and\n",
            "practical ---> pract\n",
            "skills ---> skil\n",
            ". ---> .\n",
            "With ---> with\n",
            "modern ---> modern\n",
            "laboratories ---> lab\n",
            ", ---> ,\n",
            "libraries ---> libr\n",
            ", ---> ,\n",
            "and ---> and\n",
            "research ---> research\n",
            "facilities ---> facil\n",
            ", ---> ,\n",
            "NIT ---> nit\n",
            "provides ---> provid\n",
            "a ---> a\n",
            "good ---> good\n",
            "learning ---> learn\n",
            "environment ---> environ\n",
            ". ---> .\n",
            "Students ---> stud\n",
            "from ---> from\n",
            "different ---> diff\n",
            "parts ---> part\n",
            "of ---> of\n",
            "the ---> the\n",
            "country ---> country\n",
            "study ---> study\n",
            "together ---> togeth\n",
            ", ---> ,\n",
            "promoting ---> promot\n",
            "unity ---> un\n",
            "and ---> and\n",
            "cultural ---> cult\n",
            "diversity ---> divers\n",
            ". ---> .\n",
            "Overall ---> overal\n",
            ", ---> ,\n",
            "NIT ---> nit\n",
            "College ---> colleg\n",
            "plays ---> play\n",
            "an ---> an\n",
            "important ---> import\n",
            "role ---> rol\n",
            "in ---> in\n",
            "shaping ---> shap\n",
            "skilled ---> skil\n",
            "professionals ---> profess\n",
            "and ---> and\n",
            "future ---> fut\n",
            "leaders ---> lead\n",
            "of ---> of\n",
            "the ---> the\n",
            "nation ---> nat\n",
            ". ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "regexp = RegexpStemmer('ing|s|e|able', min=4)\n",
        "words = word_tokenize(NIT)\n",
        "for word in words:\n",
        "    print(word,\"--->\",Lanc.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeoVsFu-Exzp",
        "outputId": "63da4270-55d7-4e5c-8604-7bc7aba063f7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NIT ---> nit\n",
            "College ---> colleg\n",
            ", ---> ,\n",
            "also ---> also\n",
            "known ---> known\n",
            "as ---> as\n",
            "the ---> the\n",
            "National ---> nat\n",
            "Institute ---> institut\n",
            "of ---> of\n",
            "Technology ---> technolog\n",
            ", ---> ,\n",
            "is ---> is\n",
            "one ---> on\n",
            "of ---> of\n",
            "the ---> the\n",
            "most ---> most\n",
            "prestigious ---> prestigy\n",
            "engineering ---> engin\n",
            "institutions ---> institut\n",
            "in ---> in\n",
            "India ---> ind\n",
            ". ---> .\n",
            "It ---> it\n",
            "is ---> is\n",
            "known ---> known\n",
            "for ---> for\n",
            "its ---> it\n",
            "high ---> high\n",
            "academic ---> academ\n",
            "standards ---> standard\n",
            ", ---> ,\n",
            "experienced ---> expery\n",
            "faculty ---> facul\n",
            ", ---> ,\n",
            "and ---> and\n",
            "excellent ---> excel\n",
            "infrastructure ---> infrastruct\n",
            ". ---> .\n",
            "NIT ---> nit\n",
            "colleges ---> colleg\n",
            "offer ---> off\n",
            "quality ---> qual\n",
            "education ---> educ\n",
            "in ---> in\n",
            "engineering ---> engin\n",
            ", ---> ,\n",
            "science ---> sci\n",
            ", ---> ,\n",
            "and ---> and\n",
            "technology ---> technolog\n",
            ", ---> ,\n",
            "helping ---> help\n",
            "students ---> stud\n",
            "develop ---> develop\n",
            "strong ---> strong\n",
            "technical ---> techn\n",
            "and ---> and\n",
            "practical ---> pract\n",
            "skills ---> skil\n",
            ". ---> .\n",
            "With ---> with\n",
            "modern ---> modern\n",
            "laboratories ---> lab\n",
            ", ---> ,\n",
            "libraries ---> libr\n",
            ", ---> ,\n",
            "and ---> and\n",
            "research ---> research\n",
            "facilities ---> facil\n",
            ", ---> ,\n",
            "NIT ---> nit\n",
            "provides ---> provid\n",
            "a ---> a\n",
            "good ---> good\n",
            "learning ---> learn\n",
            "environment ---> environ\n",
            ". ---> .\n",
            "Students ---> stud\n",
            "from ---> from\n",
            "different ---> diff\n",
            "parts ---> part\n",
            "of ---> of\n",
            "the ---> the\n",
            "country ---> country\n",
            "study ---> study\n",
            "together ---> togeth\n",
            ", ---> ,\n",
            "promoting ---> promot\n",
            "unity ---> un\n",
            "and ---> and\n",
            "cultural ---> cult\n",
            "diversity ---> divers\n",
            ". ---> .\n",
            "Overall ---> overal\n",
            ", ---> ,\n",
            "NIT ---> nit\n",
            "College ---> colleg\n",
            "plays ---> play\n",
            "an ---> an\n",
            "important ---> import\n",
            "role ---> rol\n",
            "in ---> in\n",
            "shaping ---> shap\n",
            "skilled ---> skil\n",
            "professionals ---> profess\n",
            "and ---> and\n",
            "future ---> fut\n",
            "leaders ---> lead\n",
            "of ---> of\n",
            "the ---> the\n",
            "nation ---> nat\n",
            ". ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = word_tokenize(NIT)\n",
        "for word in words:\n",
        "    print(word,\"--->\",lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBQ_pRDHE_z2",
        "outputId": "bce710bc-6a40-46ac-98bc-a54335df361c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NIT ---> NIT\n",
            "College ---> College\n",
            ", ---> ,\n",
            "also ---> also\n",
            "known ---> known\n",
            "as ---> a\n",
            "the ---> the\n",
            "National ---> National\n",
            "Institute ---> Institute\n",
            "of ---> of\n",
            "Technology ---> Technology\n",
            ", ---> ,\n",
            "is ---> is\n",
            "one ---> one\n",
            "of ---> of\n",
            "the ---> the\n",
            "most ---> most\n",
            "prestigious ---> prestigious\n",
            "engineering ---> engineering\n",
            "institutions ---> institution\n",
            "in ---> in\n",
            "India ---> India\n",
            ". ---> .\n",
            "It ---> It\n",
            "is ---> is\n",
            "known ---> known\n",
            "for ---> for\n",
            "its ---> it\n",
            "high ---> high\n",
            "academic ---> academic\n",
            "standards ---> standard\n",
            ", ---> ,\n",
            "experienced ---> experienced\n",
            "faculty ---> faculty\n",
            ", ---> ,\n",
            "and ---> and\n",
            "excellent ---> excellent\n",
            "infrastructure ---> infrastructure\n",
            ". ---> .\n",
            "NIT ---> NIT\n",
            "colleges ---> college\n",
            "offer ---> offer\n",
            "quality ---> quality\n",
            "education ---> education\n",
            "in ---> in\n",
            "engineering ---> engineering\n",
            ", ---> ,\n",
            "science ---> science\n",
            ", ---> ,\n",
            "and ---> and\n",
            "technology ---> technology\n",
            ", ---> ,\n",
            "helping ---> helping\n",
            "students ---> student\n",
            "develop ---> develop\n",
            "strong ---> strong\n",
            "technical ---> technical\n",
            "and ---> and\n",
            "practical ---> practical\n",
            "skills ---> skill\n",
            ". ---> .\n",
            "With ---> With\n",
            "modern ---> modern\n",
            "laboratories ---> laboratory\n",
            ", ---> ,\n",
            "libraries ---> library\n",
            ", ---> ,\n",
            "and ---> and\n",
            "research ---> research\n",
            "facilities ---> facility\n",
            ", ---> ,\n",
            "NIT ---> NIT\n",
            "provides ---> provides\n",
            "a ---> a\n",
            "good ---> good\n",
            "learning ---> learning\n",
            "environment ---> environment\n",
            ". ---> .\n",
            "Students ---> Students\n",
            "from ---> from\n",
            "different ---> different\n",
            "parts ---> part\n",
            "of ---> of\n",
            "the ---> the\n",
            "country ---> country\n",
            "study ---> study\n",
            "together ---> together\n",
            ", ---> ,\n",
            "promoting ---> promoting\n",
            "unity ---> unity\n",
            "and ---> and\n",
            "cultural ---> cultural\n",
            "diversity ---> diversity\n",
            ". ---> .\n",
            "Overall ---> Overall\n",
            ", ---> ,\n",
            "NIT ---> NIT\n",
            "College ---> College\n",
            "plays ---> play\n",
            "an ---> an\n",
            "important ---> important\n",
            "role ---> role\n",
            "in ---> in\n",
            "shaping ---> shaping\n",
            "skilled ---> skilled\n",
            "professionals ---> professional\n",
            "and ---> and\n",
            "future ---> future\n",
            "leaders ---> leader\n",
            "of ---> of\n",
            "the ---> the\n",
            "nation ---> nation\n",
            ". ---> .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"better\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z-RmKAgtFIPj",
        "outputId": "71bb0f38-3742-4adb-925c-a71ae9529d4c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'better'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"better\", pos=\"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f7K4x6__FNIc",
        "outputId": "1d91448d-7a7e-404a-f609-c587f466f83f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer, WordNetLemmatizer\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(language='english')\n",
        "regexp = RegexpStemmer('ing|s|e|able', min=4)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\"]\n",
        "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}{5:50}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer','WordNetLemmatizer'))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}{5:50}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word),lemmatizer.lemmatize(word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fnd8FPiFQFD",
        "outputId": "abae7c74-4480-45c6-efa6-f51459f56c6a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word                Porter Stemmer      Snowball Stemmer    Lancaster Stemmer             Regexp Stemmer                          WordNetLemmatizer                                 \n",
            "friend              friend              friend              friend                        frind                                   friend                                            \n",
            "friendship          friendship          friendship          friend                        frindhip                                friendship                                        \n",
            "friends             friend              friend              friend                        frind                                   friend                                            \n",
            "friendships         friendship          friendship          friend                        frindhip                                friendship                                        \n"
          ]
        }
      ]
    }
  ]
}